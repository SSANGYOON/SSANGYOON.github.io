---
layout : single
title : "부스트캠프 7일차- pytorch basics"
---

## 1. pytorch

DBG(Define by run) 방식의 프레임워크  
tensorflow(1.x)와 달리 실행 시간에 그래프가 생성됨

## 2. torch

pytorch에서 사용하는 numpy 형식의 데이터(tensor) 제공  
backpropagation 과정에서 자동미분을 제공함  
다양한 기능을 가진 함수와 모델 제공  

## 3. tensor

numpy와 비슷한 형태의 데이터 비슷한 원리로 생성 및 사용이 가능하다

```python
   a_numpy = np.array([1,2,3,4])
   a_tensor_from_numpy = torch.from_numpy(a_numpy)
   a_tensor = torch.Tensor([1,2,3,4])
```

기본적으로 numpy와 비슷한 형태의 인덱싱이 가능하다

```python
   data = [[1,2,3],[4,5,6],[7,8,9]]
   tensor_from_data=torch.from_numpy(data)
   data[:,0]
   #array([[1],[4],[7]],dtype=int64)
   tensor_from_data[:,0]
   #tensor([[1],[4],[7]])
```

torch는 gpu에 올려서 gpu를 사용가능하다

```python
    if torch.cuda.is_available():
       data_cuda = tensor_from_data.to('cuda')
    x_data_cuda.device
   # device(type = 'cuda', index = 0)
```

## 4. tensor operations  

### tensor의 인덱싱
```python
    tensor_ex = torch.rand(size=(2, 3, 2))
    tensor_ex
    # tensor([[[0.7466, 0.5440],
    # [0.7145, 0.2119],
    # [0.8279, 0.0697]],
    # [[0.8323, 0.2671],
    # [0.2484, 0.8983],
    # [0.3228, 0.2254]]])
    tensor_ex.view([-1, 6])
    # tensor([[0.7466, 0.5440, 0.7145, 0.2119, 0.8279, 0.0697],
    # [0.8323, 0.2671, 0.2484, 0.8983, 0.3228, 0.2254]])
    tensor_ex.reshape([-1,6])
    # tensor([[0.7466, 0.5440, 0.7145, 0.2119, 0.8279, 0.0697],
    # [0.8323, 0.2671, 0.2484, 0.8983, 0.3228, 0.2254]])
```

### view, reshape ,squeeze, unsqueeze  
결과가 같아 보이지만  
view는 원래의 메모리 공간을 공유하고(원본이 바뀌면 자신도 바뀜)  
reshape는 새로운 텐서를 생성한다는 차이가 있다  
squeeze는 크기가 1인 차원을 제거
unsqueeze는 크기가 1인 차원을 인풋으로 준 위치에 추가한다
```python
    tensor_ex = torch.rand(size=(2, 1, 2))
    tensor_ex.squeeze()
    # tensor([[0.8510, 0.8263],
    # [0.7602, 0.1309]])
    tensor_ex = torch.rand(size=(2, 2))
    tensor_ex.unsqueeze(0).shape
    # torch.Size([1, 2, 2])
    tensor_ex.unsqueeze(1).shape
    # torch.Size([2, 1, 2])
    tensor_ex.unsqueeze(2).shape
    # torch.Size([2, 2, 1]) 
```
### 텐서 연산
기본적인 연산은 numpy와 동일하고 마찬가지로 broadcasting을 제공한다
그러나 행렬 곱셈에서 dot 대신 mm(matrix multiplication)을 쓴다 

```python
    n2 = np.arange(10).reshape(5,2)
    t2 = torch.FloatTensor(n2)
    t1.mm(t2)
    # tensor([[ 60., 70.],
    # [160., 195.]])
    t1.dot(t2)
    # RuntimeError
    t1.matmul(t2)
    # tensor([[ 60., 70.],
    # [160., 195.]])
    a = torch.rand(10)
    b = torch.rand(10)
    a.dot(b)
    a = torch.rand(10)
    b = torch.rand(10)
    a.mm(b)
```

## 4. autograd

텐서의backward 함수를 사용하여 텐서의 그레디언트를 계산할 수 있다.

```python
    a = torch.tensor([2., 3.], requires_grad=True)
    b = torch.tensor([6., 4.], requires_grad=True)
    Q = 3*a**3 - b**2
    external_grad = torch.tensor([1., 1.])
    Q.backward(gradient=external_grad)
    a.grad
    # a.grad
    b.grad
    # tensor([-12., -8.])
```

오늘 몸무게 92.4 kg 힘내자!!